{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "870d965d-103c-4ccd-8b92-6c3f4fd11fb3",
   "metadata": {},
   "source": [
    "#### 1. Implement all activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4703ea32-d515-4514-a682-f3add86b8bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid Activation:\n",
      "[0.57444252 0.5938731  0.61301418]\n",
      "\n",
      "Tanh Activation:\n",
      "[1.54186539 1.67847957 1.81683859]\n",
      "\n",
      "ReLU Activation:\n",
      "[0.3  0.38 0.46]\n",
      "\n",
      "Step Function Activation:\n",
      "[1 1 1]\n",
      "\n",
      "Softmax Output:\n",
      "[0.36949149 0.63050851]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(X):\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "\n",
    "def tanh(X):\n",
    "    return (np.exp(X)-np.exp(-X)/np.exp(X)+np.exp(-X))\n",
    "\n",
    "def relu(X):\n",
    "    return np.maximum(0, X)\n",
    "\n",
    "def step_function(X):\n",
    "    return np.where(X > 0, 1, 0)\n",
    "\n",
    "def softmax(X): \n",
    "    exp_x = np.exp(X - np.max(X))\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "X = np.array([0.5, 0.2, 0.1]) # 3 input neuron\n",
    "W1 = np.array([[0.1, 0.2, 0.3], # 1st layer weight\n",
    "               [0.4, 0.5, 0.6],\n",
    "               [0.7, 0.8, 0.9]])\n",
    "b1 = np.array([0.1, 0.1, 0.1]) # Bias for 1st layer\n",
    "W2 = np.array([[0.1, 0.4], # 2nd layer weight\n",
    "               [0.2, 0.5],\n",
    "               [0.3, 0.6]])\n",
    "b2 = np.array([0.2, 0.2]) # Bias for 2nd layer\n",
    "\n",
    "Z1 = np.dot(X, W1) + b1\n",
    "A1 = {\n",
    "    \"Sigmoid\": sigmoid(Z1),\n",
    "    \"Tanh\": tanh(Z1),\n",
    "    \"ReLU\": relu(Z1),\n",
    "    \"Step Function\": step_function(Z1)\n",
    "}\n",
    "\n",
    "Z2 = np.dot(A1[\"Sigmoid\"], W2) + b2\n",
    "A2 = softmax(Z2)\n",
    "\n",
    "for name, y in A1.items():\n",
    "    print(f\"{name} Activation:\")\n",
    "    print(y)\n",
    "    print()\n",
    "\n",
    "print(\"Softmax Output:\")\n",
    "print(A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486d5071-087a-4279-b86d-eb69253c03ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c406ca12-4512-4e17-8015-66d44a48b859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "951e049a-4e11-421f-a4a8-433abe88673b",
   "metadata": {},
   "source": [
    "#### Q) Implement a two-layer perceptron with sigmoid activation in the hidden layer and softmax in the output layer. Use it for a simple classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd91c9d-e165-4353-a8dd-e4a653848cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summation at Hidden Layer (Z1): [0.4  0.48 0.56]\n",
      "Output of Hidden Layer (A1): [0.59868766 0.61774787 0.63645254]\n",
      "Summation at Output Layer (Z2): [0.6743541  1.23022053]\n",
      "Final Output (A2): [0.36450443 0.63549557]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "# Layer 1\n",
    "X = np.array([0.5, 0.2, 0.1])\n",
    "W1 = np.array([[0.1, 0.2, 0.3], \n",
    "               [0.4, 0.5, 0.6], \n",
    "               [0.7, 0.8, 0.9]])\n",
    "b1 = np.array([0.2, 0.2, 0.2])\n",
    "\n",
    "#  Output Layer 2\n",
    "W2 = np.array([[0.1, 0.4], \n",
    "               [0.2, 0.5], \n",
    "               [0.3, 0.6]])\n",
    "b2 = np.array([0.3, 0.3])\n",
    "\n",
    "Z1 = np.dot(X, W1) + b1\n",
    "print(\"Summation at Hidden Layer (Z1):\",Z1)\n",
    "A1 = sigmoid(Z1)\n",
    "print(\"Output of Hidden Layer (A1):\",A1)\n",
    "\n",
    "Z2 = np.dot(A1, W2) + b2\n",
    "print(\"Summation at Output Layer (Z2):\",Z2)\n",
    "A2 = softmax(Z2)\n",
    "print(\"Final Output (A2):\",A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01edfe8-cdca-4faf-934d-fc360a4a1a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a327251-ff44-4e12-8de4-afb1974dd4ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
