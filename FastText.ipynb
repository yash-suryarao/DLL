{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2d72af4-0d61-4481-a178-aecff69bb8de",
   "metadata": {},
   "source": [
    "#### Q. WAP using pre-trained FastText embedding to solve the analogy: 'France':'Paris'::'Germany'::'____'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af824b1-859a-4c53-9f04-02801d1ba483",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a668960-b246-4255-9428-ebdb9ec51e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FastText model... Please wait...\n",
      "[--------------------------------------------------] 0.1% 1.2/958.4MB downloaded"
     ]
    }
   ],
   "source": [
    "# Q2. WAP using pre-trained FastText embedding to solve the analogy: 'France':'Paris'::'Germany'::'____'\n",
    "from gensim.models import FastText\n",
    "from gensim.downloader import load\n",
    "\n",
    "# 1. Load pretrained FastText embeddings\n",
    "print(\"Loading FastText model... Please wait...\")\n",
    "model = load('fasttext-wiki-news-subwords-300')  # Uses Gensim's built-in FastText\n",
    "\n",
    "# 2. Define analogy\n",
    "positive = ['Paris', 'Germany']\n",
    "negative = ['France']\n",
    "\n",
    "# 3. Solve analogy\n",
    "result = model.most_similar(positive=positive, negative=negative, topn=1)\n",
    "\n",
    "# 4. Get the top result\n",
    "word, similarity = result[0]\n",
    "\n",
    "# 5. Print in the desired format\n",
    "print(f\"\\n'France' : 'Paris' :: 'Germany' : '{word}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad11c66-93ad-437e-b738-2028d0aafd91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68830bc3-2bdd-460e-a0fb-aaf32e56314c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2922a4eb-1b6b-439e-863e-9cb9cb3ad6d3",
   "metadata": {},
   "source": [
    "#### Q. WAP to find top 7 most similar to the given word use FastText embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889b1493-2922-4449-b5b3-a05fcd892f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. WAP to find top 7 most similar to the given word use FastText embedding\n",
    "\n",
    "# THIS PROGRAM IS FOR ANALOGY\n",
    "from gensim.models import FastText\n",
    "from gensim.downloader import load\n",
    "\n",
    "# 1. Load pre-trained FastText word vectors\n",
    "print(\"Loading pre-trained FastText model... (please wait)\")\n",
    "model = load('fasttext-wiki-news-subwords-300')  # FastText trained on Wikipedia + subwords\n",
    "\n",
    "# 2. Solve the analogy: France : Paris :: Germany : ?\n",
    "positive_words = ['Paris', 'Germany']\n",
    "negative_words = ['France']\n",
    "\n",
    "# 3. Perform analogy calculation\n",
    "result = model.most_similar(positive=positive_words, negative=negative_words, topn=7)\n",
    "\n",
    "# 4. Display the top results\n",
    "print(\"\\nAnalogy: 'France' : 'Paris' :: 'Germany' : -->\\n\")\n",
    "for word, score in result:\n",
    "    print(f\"{word:15s} --> similarity: {score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
