{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "627bda82-f985-4145-9d64-55d809346f88",
   "metadata": {},
   "source": [
    "#### 1. Write a Python function to remove stopwords and punctuation from a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "680975a5-84b6-4e73-9a73-300d9c6ec6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python function remove stopwords punctuation given text\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize the text\n",
    "    words = text.split()\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Join words back into a string\n",
    "    cleaned_text = ' '.join(filtered_words)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "sample_text = \"Python function, to remove stopwords and punctuation from a given text.\"\n",
    "print(clean_text(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe531eb-d011-4a0c-8bbf-499e4eda835c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6102e3f0-7009-4834-b11a-cb6ada66ff26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "860ca29e-6891-4fe7-adf2-2676878f9f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\yash\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\yash\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.11.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\yash\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\yash\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/24.0 MB 4.2 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 2.6/24.0 MB 6.6 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 5.2/24.0 MB 8.9 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 7.6/24.0 MB 9.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 10.2/24.0 MB 10.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 12.3/24.0 MB 10.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 15.7/24.0 MB 10.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 18.4/24.0 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 20.7/24.0 MB 11.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.1/24.0 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.0 MB 11.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.0 MB 11.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.0 MB 11.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.0 MB 11.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.0 MB 11.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 7.1 MB/s eta 0:00:00\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.3.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\yash\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\mask_rcnn-2.1-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf6f2c5-2323-4d50-81ae-f28917de7d1f",
   "metadata": {},
   "source": [
    "### 2. Train a Word2Vec model on a given dataset and display the most similar words to a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a14aec35-4788-457e-8d65-e614aef356b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Yash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4597b56-0da6-44f4-bb56-70324644fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe3231c5-b224-47b7-a583-57a53786778c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Cleaned Text: ['sample', 'sentence', 'stopwords', 'punctuation']\n"
     ]
    }
   ],
   "source": [
    "# 1. Remove stopwords and punctuation\n",
    "\n",
    "def clean_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "    cleaned = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    return cleaned\n",
    "\n",
    "sample_text = \"This is a sample sentence, with some stopwords and punctuation!\"\n",
    "cleaned_words = clean_text(sample_text)\n",
    "print(\"1. Cleaned Text:\", cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8ea4269-908a-43b5-acff-be2636a365af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Words similar to 'learning':\n",
      "[('processing', 0.21617142856121063), ('machine', 0.09310110658407211), ('embeddings', 0.09291724115610123), ('natural', 0.0796348825097084), ('language', 0.06285079568624496), ('word', 0.027057485654950142), ('fun', 0.016134684905409813), ('text', -0.010839172638952732), ('deep', -0.027756884694099426), ('love', -0.04125342518091202)]\n"
     ]
    }
   ],
   "source": [
    "# 2. Train Word2Vec and find similar words\n",
    "\n",
    "def train_word2vec(sentences):\n",
    "    model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    return model\n",
    "\n",
    "def most_similar_words(model, word):\n",
    "    if word in model.wv:\n",
    "        return model.wv.most_similar(word)\n",
    "    else:\n",
    "        return f\"'{word}' not in vocabulary.\"\n",
    "\n",
    "# Tokenized sentences for training\n",
    "tokenized_sentences = [\n",
    "    clean_text(\"I love machine learning and natural language processing.\"),\n",
    "    clean_text(\"Word embeddings are fun to work with.\"),\n",
    "    clean_text(\"Deep learning models can understand text.\")\n",
    "]\n",
    "\n",
    "w2v_model = train_word2vec(tokenized_sentences)\n",
    "similar = most_similar_words(w2v_model, \"learning\")\n",
    "print(\"\\n2. Words similar to 'learning':\")\n",
    "print(similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49a0357-b062-4b1f-a779-b52565ee8b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adecb5db-e853-4d73-a600-d05ae67a7c81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57550e2d-8e30-4d44-804b-5c83846979c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2369939-4815-4d1b-a2cc-3029816092c2",
   "metadata": {},
   "source": [
    "#### 3. WAP to convert a set of sentence into TF-IDF vectors using TfidfVectorizer from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97095084-0eaa-4390-b5cd-a4e62230360b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. TF-IDF Feature Names:\n",
      "['analysis' 'great' 'idf' 'in' 'is' 'language' 'learning' 'machine'\n",
      " 'natural' 'of' 'part' 'processing' 'text' 'tf' 'used']\n",
      "TF-IDF Matrix:\n",
      "[[0.         0.63174505 0.         0.         0.37311881 0.\n",
      "  0.4804584  0.4804584  0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.2315585  0.39206263\n",
      "  0.29817373 0.29817373 0.39206263 0.39206263 0.39206263 0.39206263\n",
      "  0.         0.         0.        ]\n",
      " [0.39687454 0.         0.39687454 0.39687454 0.2344005  0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.39687454 0.39687454 0.39687454]]\n"
     ]
    }
   ],
   "source": [
    "# 3. TF-IDF Vectorizer\n",
    "\n",
    "def tfidf_vectorize(sentences):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    return tfidf_matrix, vectorizer.get_feature_names_out()\n",
    "\n",
    "tfidf_sentences = [\n",
    "    \"Machine learning is great.\",\n",
    "    \"Natural language processing is a part of machine learning.\",\n",
    "    \"TF-IDF is used in text analysis.\"\n",
    "]\n",
    "\n",
    "tfidf_matrix, features = tfidf_vectorize(tfidf_sentences)\n",
    "print(\"\\n3. TF-IDF Feature Names:\")\n",
    "print(features)\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaa9ff2e-b0a7-494b-957a-f21a006d02dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step\n",
      "\n",
      "4. Encoded Data (first sentence):\n",
      "[0.         0.         0.17588207 0.01944692 0.         0.\n",
      " 0.6262368  0.21444389 0.         0.34238204 0.06607856 0.36305642\n",
      " 0.         0.07626814 0.         0.28467283 0.         0.\n",
      " 0.         0.         0.         0.21185587 0.         0.\n",
      " 0.26284075 0.         0.         0.01295196 0.15640222 0.\n",
      " 0.         0.        ]\n",
      "Reconstructed Data (first sentence):\n",
      "[0.34767815 0.51846886 0.5067942  0.44457883 0.42402798 0.41451383\n",
      " 0.4304292  0.43372533 0.44822305 0.47661552 0.4466941  0.3734453\n",
      " 0.4506051  0.43776143 0.39824685]\n"
     ]
    }
   ],
   "source": [
    "# 4. Autoencoder encode and reconstruct\n",
    "\n",
    "def build_autoencoder(input_dim):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoded = Dense(32, activation='relu')(input_layer)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "    return autoencoder, encoder\n",
    "\n",
    "def encode_and_reconstruct(autoencoder, encoder, data):\n",
    "    encoded_data = encoder.predict(data)\n",
    "    reconstructed_data = autoencoder.predict(data)\n",
    "    return encoded_data, reconstructed_data\n",
    "\n",
    "# Use TF-IDF matrix as input data\n",
    "input_data = tfidf_matrix.toarray()\n",
    "input_dim = input_data.shape[1]\n",
    "\n",
    "autoencoder, encoder = build_autoencoder(input_dim)\n",
    "autoencoder.fit(input_data, input_data, epochs=50, verbose=0)\n",
    "\n",
    "encoded_data, reconstructed_data = encode_and_reconstruct(autoencoder, encoder, input_data)\n",
    "\n",
    "print(\"\\n4. Encoded Data (first sentence):\")\n",
    "print(encoded_data[0])\n",
    "print(\"Reconstructed Data (first sentence):\")\n",
    "print(reconstructed_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6727df95-e4bb-4c52-9098-7818f3e4ae5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f09b575b-ec9d-4503-b369-5f6e8f0d99af",
   "metadata": {},
   "source": [
    "#### 3. Write a program to convert a set of sentences into TF-IDF vectors using TfidfVectorizer from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac7993a4-f25b-4023-b7e2-49c0b1b3af49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names:\n",
      "['amazing' 'analysis' 'idf' 'in' 'is' 'language' 'learning' 'machine'\n",
      " 'natural' 'of' 'part' 'processing' 'text' 'tf' 'used']\n",
      "\n",
      "TF-IDF Vectors:\n",
      "Sentence 1: [0.63174505 0.         0.         0.         0.37311881 0.\n",
      " 0.4804584  0.4804584  0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "Sentence 2: [0.         0.         0.         0.         0.2315585  0.39206263\n",
      " 0.29817373 0.29817373 0.39206263 0.39206263 0.39206263 0.39206263\n",
      " 0.         0.         0.        ]\n",
      "Sentence 3: [0.         0.39687454 0.39687454 0.39687454 0.2344005  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.39687454 0.39687454 0.39687454]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sentences = [\n",
    "    \"Machine learning is amazing.\",\n",
    "    \"Natural language processing is a part of machine learning.\",\n",
    "    \"TF-IDF is used in text analysis.\"\n",
    "]\n",
    "\n",
    "# Create a TF-IDF Vectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the sentences\n",
    "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Get feature names (i.e., unique words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert TF-IDF matrix to array (optional)\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "# Print feature names\n",
    "print(\"Feature Names:\")\n",
    "print(feature_names)\n",
    "\n",
    "# Print TF-IDF matrix\n",
    "print(\"\\nTF-IDF Vectors:\")\n",
    "for i, vector in enumerate(tfidf_array):\n",
    "    print(f\"Sentence {i+1}: {vector}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f558781-040a-4752-b1a4-bb6c54788373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a85b77b-42f1-4fcd-8f8f-a8d95fcb5fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba835886-78ae-4e53-946a-df1c2a3bddf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a88986df-e7a3-49c4-a6bb-e3b9565d2922",
   "metadata": {},
   "source": [
    "#### 4. Write a function that encodes an input using an autoencoder and then reconstructs it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abc2ad4b-1343-4a08-98f8-6d8fcf5d9bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "    amazing  analysis       idf        in        is  language  learning  \\\n",
      "0  0.631745  0.000000  0.000000  0.000000  0.373119  0.000000  0.480458   \n",
      "1  0.000000  0.000000  0.000000  0.000000  0.231559  0.392063  0.298174   \n",
      "2  0.000000  0.396875  0.396875  0.396875  0.234400  0.000000  0.000000   \n",
      "\n",
      "    machine   natural        of      part  processing      text        tf  \\\n",
      "0  0.480458  0.000000  0.000000  0.000000    0.000000  0.000000  0.000000   \n",
      "1  0.298174  0.392063  0.392063  0.392063    0.392063  0.000000  0.000000   \n",
      "2  0.000000  0.000000  0.000000  0.000000    0.000000  0.396875  0.396875   \n",
      "\n",
      "       used  \n",
      "0  0.000000  \n",
      "1  0.000000  \n",
      "2  0.396875  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Input sentences\n",
    "sentences = [\n",
    "    \"Machine learning is amazing.\",\n",
    "    \"Natural language processing is a part of machine learning.\",\n",
    "    \"TF-IDF is used in text analysis.\"\n",
    "]\n",
    "\n",
    "# Step 2: Create a TfidfVectorizer instance\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Step 3: Fit and transform the sentences into TF-IDF vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Step 4: Convert to a DataFrame for readability\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Step 5: Display the TF-IDF DataFrame\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de08d092-4577-406d-9a77-5422d93ca5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('fox', 0.25296375155448914), ('this', 0.200852170586586), ('green', 0.17530228197574615), ('and', 0.17030838131904602), ('brown', 0.15023493766784668)]\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "\n",
    "def train_word2vec_model(text_data, vector_size=100, window=5, min_count=1):\n",
    "    \n",
    "    # Tokenize each sentence into words\n",
    "    tokenized_data = [word_tokenize(sentence.lower()) for sentence in text_data]\n",
    "\n",
    "    # Train the Word2Vec model\n",
    "    model = Word2Vec(sentences=tokenized_data, vector_size=vector_size, window=window, min_count=min_count)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_similar_words(model, word, topn=5):\n",
    "    \n",
    "    if word in model.wv:\n",
    "        return model.wv.most_similar(word, topn=topn)\n",
    "    else:\n",
    "        return f\"'{word}' not in vocabulary.\"\n",
    "\n",
    "# Example usage\n",
    "text_data = [\n",
    "    \"The sky is blue and beautiful.\",\n",
    "    \"Love this blue and beautiful sky!\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
    "    \"I love green eggs, ham, sausages and bacon!\",\n",
    "    \"The brown fox is quick and the blue dog is lazy!\",\n",
    "]\n",
    "\n",
    "model = train_word2vec_model(text_data)\n",
    "similar_words = get_similar_words(model, \"blue\")\n",
    "print(similar_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
