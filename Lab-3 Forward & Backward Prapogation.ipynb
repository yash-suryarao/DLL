{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8cb328f-fc2f-4b74-a60f-2cde9595c7d5",
   "metadata": {},
   "source": [
    "#### Forward Till MSE Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "361a5117-b655-414c-bc14-0f59423ae1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summation at Hidden Layer (Z1): [0.3  0.48 0.66]\n",
      "Output of Hidden Layer (A1): [0.57444252 0.61774787 0.65926039]\n",
      "Summation at Output Layer (Z2): [0.47877194 1.13420718]\n",
      "Final Output (A2): [0.61745785 0.75661448]\n",
      "Mean Squared Error Loss: 0.3594019879097132\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# MSE Function\n",
    "def mean_square_error(predicted, actual):\n",
    "    return np.sum(np.square(predicted - actual))/2\n",
    "\n",
    "# Layer 1\n",
    "X = np.array([0.5, 0.2, 0.1])\n",
    "W1 = np.array([[0.1, 0.2, 0.3], \n",
    "               [0.4, 0.5, 0.6], \n",
    "               [0.7, 0.8, 0.9]])\n",
    "b1 = np.array([0.1, 0.2, 0.3])\n",
    "\n",
    "#  Output Layer 2\n",
    "W2 = np.array([[0.1, 0.4], \n",
    "               [0.2, 0.5], \n",
    "               [0.3, 0.6]])\n",
    "b2 = np.array([0.1, 0.2])\n",
    "\n",
    "Z1 = np.dot(X, W1) + b1\n",
    "print(\"Summation at Hidden Layer (Z1):\",Z1)\n",
    "A1 = sigmoid(Z1)\n",
    "print(\"Output of Hidden Layer (A1):\",A1)\n",
    "\n",
    "Z2 = np.dot(A1, W2) + b2\n",
    "print(\"Summation at Output Layer (Z2):\",Z2)\n",
    "A2 = sigmoid(Z2)\n",
    "print(\"Final Output (A2):\",A2)\n",
    "\n",
    "Y = np.array([1, 0])\n",
    "\n",
    "# Compute MSE Loss\n",
    "loss = mean_square_error(A2, Y)\n",
    "print(\"Mean Squared Error Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a50aa1-d451-47f1-9a7b-9383fa29c608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0536752-db19-4a7b-b567-3beddd680e6d",
   "metadata": {},
   "source": [
    "#### Feed-Forward Propagation\n",
    "##### 3 input neuron\n",
    "##### 3 Hidden Layer(4 neuron in each layer)\n",
    "##### 1 output neuron\n",
    "##### Use Tanh Activation Function and Sigmoid Activation Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04eb5b14-d8df-48dc-86ab-dfed2a25c22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (A0) :  [0.6 0.8 0.3]\n",
      "\n",
      "Layer 1 (Hidden Layer):\n",
      "Z1: [0.82 0.85 1.17 0.87]\n",
      "A1 (after tanh): [0.67506987 0.69106947 0.82427217 0.70137413]\n",
      "\n",
      "Layer 2 (Hidden Layer):\n",
      "Z2: [1.66529891 1.79111448 1.65877425 1.59875719]\n",
      "A2 (after tanh): [0.93092742 0.94587806 0.93005189 0.92148126]\n",
      "\n",
      "Layer 3 (Hidden Layer):\n",
      "Z3: [2.33319994 1.87676904 1.78308225 1.39697387]\n",
      "A3 (after tanh): [0.98136316 0.95420385 0.94502569 0.88469579]\n",
      "\n",
      "Output Layer:\n",
      "Z_out: [2.19505496]\n",
      "Final Output (after sigmoid): [0.89980457]\n",
      "Final Output: [0.89980457]\n",
      "\n",
      "Mean Squared Error Loss: 0.4098436902862408\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tanh(X): # Use at Hidden layer\n",
    "    return (np.exp(X)-np.exp(-X))/(np.exp(X)+np.exp(-X))\n",
    "\n",
    "def sigmoid(X): # Use on Output layer\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "\n",
    "# MSE Function\n",
    "def mean_square_error(predicted, actual):\n",
    "    return np.sum(np.square(predicted - actual))/2\n",
    "\n",
    "# Forward Prapogation\n",
    "def feed_forward(X, weights, biases):\n",
    "    input_layer = X\n",
    "    print(\"Input (A0) : \", input_layer)\n",
    "    \n",
    "    for i in range(len(weights) -1):\n",
    "        Z = np.dot(input_layer, weights[i]) + biases[i].reshape(-1)\n",
    "        A = tanh(Z)\n",
    "        print(f\"\\nLayer {i+1} (Hidden Layer):\")\n",
    "        print(f\"Z{i+1}: {Z}\")\n",
    "        print(f\"A{i+1} (after tanh): {A}\")\n",
    "        input_layer = A\n",
    "        \n",
    "    # Output layer\n",
    "    Z_output = np.dot(input_layer, weights[-1]) + biases[-1].reshape(-1)\n",
    "    A_output = sigmoid(Z_output)\n",
    "    print(f\"\\nOutput Layer:\")\n",
    "    print(f\"Z_out: {Z_output}\")\n",
    "    print(f\"Final Output (after sigmoid): {A_output}\")\n",
    "\n",
    "    return A_output\n",
    "\n",
    "# Inputs\n",
    "X = np.array([0.6, 0.8, 0.3])\n",
    "\n",
    "weights = [\n",
    "    np.array([[0.2, 0.5, 0.3, 0.7],\n",
    "              [0.4, 0.1, 0.8, 0.2],\n",
    "              [0.6, 0.9, 0.5, 0.3]]),\n",
    "    np.array([[0.1, 0.3, 0.6, 0.8],\n",
    "              [0.5, 0.2, 0.7, 0.4],\n",
    "              [0.9, 0.8, 0.4, 0.5],\n",
    "              [0.3, 0.7, 0.2, 0.1]]),\n",
    "    np.array([[0.7, 0.5, 0.4, 0.2],\n",
    "              [0.3, 0.8, 0.6, 0.1],\n",
    "              [0.9, 0.2, 0.7, 0.3],\n",
    "              [0.5, 0.4, 0.1, 0.8]]),\n",
    "    np.array([[0.6],\n",
    "              [0.2],\n",
    "              [0.7],\n",
    "              [0.4]])\n",
    "]\n",
    "\n",
    "biases = [\n",
    "    np.array([0.2, 0.2, 0.2, 0.2]),\n",
    "    np.array([0.3, 0.3, 0.3, 0.3]),\n",
    "    np.array([0.1, 0.1, 0.1, 0.1]),\n",
    "    np.array([0.4])\n",
    "]\n",
    "\n",
    "\n",
    "output = feed_forward(X, weights, biases)\n",
    "print(\"Final Output:\", output)\n",
    "\n",
    "Y = np.array([1, 0])\n",
    "\n",
    "# Compute MSE Loss\n",
    "loss = mean_square_error(output, Y)\n",
    "print(\"\\nMean Squared Error Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a701fb44-38c5-42c5-9218-aa056baa9ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0adfd9e2-120c-4364-b9c8-bc588f1e102e",
   "metadata": {},
   "source": [
    "#### Backword Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd2caf11-83d7-4739-977c-468e5b851d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (A0) :  [0.6 0.8 0.3]\n",
      "\n",
      "Layer 1 (Hidden Layer):\n",
      "Z1: [0.82 0.85 1.17 0.87]\n",
      "A1 (after tanh): [0.67506987 0.69106947 0.82427217 0.70137413]\n",
      "\n",
      "Layer 2 (Hidden Layer):\n",
      "Z2: [1.66529891 1.79111448 1.65877425 1.59875719]\n",
      "A2 (after tanh): [0.93092742 0.94587806 0.93005189 0.92148126]\n",
      "\n",
      "Layer 3 (Hidden Layer):\n",
      "Z3: [2.33319994 1.87676904 1.78308225 1.39697387]\n",
      "A3 (after tanh): [0.98136316 0.95420385 0.94502569 0.88469579]\n",
      "\n",
      "Output Layer:\n",
      "Z_out: [1.99505496 2.16568196]\n",
      "Final Output (after sigmoid): [0.8802769  0.89712513]\n",
      "Final Output: [0.8802769  0.89712513]\n",
      "\n",
      "Mean Squared Error Loss: 0.4095835597653488\n",
      "\n",
      "Gradients for Weights:\n",
      "dW1:\n",
      " [[0.00688561 0.00575783 0.00480007 0.00340393]\n",
      " [0.00918081 0.00767711 0.0064001  0.00453857]\n",
      " [0.0034428  0.00287892 0.00240004 0.00170197]]\n",
      "dW2:\n",
      " [[0.00559    0.00508069 0.00628497 0.01047436]\n",
      " [0.00572249 0.0052011  0.00643393 0.01072261]\n",
      " [0.00682549 0.00620361 0.00767406 0.01278937]\n",
      " [0.00580781 0.00527866 0.00652986 0.01088249]]\n",
      "dW3:\n",
      " [[0.00612632 0.0451813  0.03271348 0.08916388]\n",
      " [0.00622471 0.04590691 0.03323886 0.09059585]\n",
      " [0.00612056 0.04513881 0.03268272 0.08908002]\n",
      " [0.00606416 0.04472284 0.03238154 0.08825913]]\n",
      "dW4:\n",
      " [[-0.10342535  0.78983394]\n",
      " [-0.10056305  0.76797522]\n",
      " [-0.09959576  0.76058833]\n",
      " [-0.09323763  0.71203281]]\n",
      "\n",
      "Gradients for Biases:\n",
      "db1:\n",
      " [0.01147602 0.00959639 0.00800012 0.00567322]\n",
      "db2:\n",
      " [0.00828062 0.00752617 0.0093101  0.01551596]\n",
      "db3:\n",
      " [0.00658088 0.04853364 0.03514075 0.09577963]\n",
      "db4:\n",
      " [-0.10538948  0.8048335 ]\n",
      "\n",
      "Updated Weights:\n",
      "W1:\n",
      " [[0.19931144 0.49942422 0.29951999 0.69965961]\n",
      " [0.39908192 0.09923229 0.79935999 0.19954614]\n",
      " [0.59965572 0.89971211 0.49976    0.2998298 ]]\n",
      "W2:\n",
      " [[0.099441   0.29949193 0.5993715  0.79895256]\n",
      " [0.49942775 0.19947989 0.69935661 0.39892774]\n",
      " [0.89931745 0.79937964 0.39923259 0.49872106]\n",
      " [0.29941922 0.69947213 0.19934701 0.09891175]]\n",
      "W3:\n",
      " [[0.69938737 0.49548187 0.39672865 0.19108361]\n",
      " [0.29937753 0.79540931 0.59667611 0.09094042]\n",
      " [0.89938794 0.19548612 0.69673173 0.291092  ]\n",
      " [0.49939358 0.39552772 0.09676185 0.79117409]]\n",
      "W4:\n",
      " [[0.61034254 0.22101661]\n",
      " [0.2100563  0.62320248]\n",
      " [0.70995958 0.42394117]\n",
      " [0.40932376 0.52879672]]\n",
      "\n",
      "Updated Biases:\n",
      "b1:\n",
      " [0.1988524  0.19904036 0.19919999 0.19943268]\n",
      "b2:\n",
      " [0.29917194 0.29924738 0.29906899 0.2984484 ]\n",
      "b3:\n",
      " [0.09934191 0.09514664 0.09648593 0.09042204]\n",
      "b4:\n",
      " [0.21053895 0.11951665]\n"
     ]
    }
   ],
   "source": [
    "# FORWARD PROPAGATION\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def tanh(X): # Use at Hidden layer\n",
    "    return (np.exp(X)-np.exp(-X))/(np.exp(X)+np.exp(-X))\n",
    "\n",
    "def sigmoid(X): # Use on Output layer\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "\n",
    "# MSE Function\n",
    "def mean_square_error(predicted, actual):\n",
    "    return np.sum(np.square(predicted - actual))/2\n",
    "\n",
    "# Forward Propagation\n",
    "def feed_forward(X, weights, biases):\n",
    "    input_layer = X\n",
    "    Zs = []\n",
    "    As = []\n",
    "    print(\"Input (A0) : \", input_layer)\n",
    "    \n",
    "    for i in range(len(weights) -1):\n",
    "        Z = np.dot(input_layer, weights[i]) + biases[i].reshape(-1)\n",
    "        A = tanh(Z)\n",
    "        print(f\"\\nLayer {i+1} (Hidden Layer):\")\n",
    "        print(f\"Z{i+1}: {Z}\")\n",
    "        print(f\"A{i+1} (after tanh): {A}\")\n",
    "        Zs.append(Z)\n",
    "        As.append(A)\n",
    "        input_layer = A\n",
    "        \n",
    "    # Output layer\n",
    "    Z_output = np.dot(input_layer, weights[-1]) + biases[-1].reshape(-1)\n",
    "    A_output = sigmoid(Z_output)\n",
    "    print(f\"\\nOutput Layer:\")\n",
    "    print(f\"Z_out: {Z_output}\")\n",
    "    print(f\"Final Output (after sigmoid): {A_output}\")\n",
    "\n",
    "    Zs.append(Z_output)\n",
    "    As.append(A_output)\n",
    "\n",
    "    return A_output, {\"Zs\":Zs, \"As\": As}\n",
    "\n",
    "\n",
    "# BACK PROPAGATION\n",
    "def back_propagation(X, Y, weights, biases, forward_cache, learning_rate):\n",
    "    Zs, As = forward_cache[\"Zs\"], forward_cache[\"As\"]\n",
    "\n",
    "    grads_W = []\n",
    "    grads_b = []\n",
    "    \n",
    "    # Output Layer Error\n",
    "    A_output = As[-1]\n",
    "    Z_output = Zs[-1]\n",
    "    \n",
    "    dA = A_output - Y\n",
    "    dZ = dA * sigmoid(Z_output)\n",
    "\n",
    "    dW = np.outer(As[-2], dZ)\n",
    "    db = dZ\n",
    "    grads_W.insert(0, dW)\n",
    "    grads_b.insert(0, db)\n",
    "\n",
    "    # Backprop through hidden layers (reversed)\n",
    "    for i in reversed(range(len(Zs) - 1)):\n",
    "        dA = np.dot(dZ, weights[i+1].T)\n",
    "        dZ = dA * (1-np.tanh(Zs[i])**2)\n",
    "        \n",
    "        A_prev = X if i == 0 else As[i-1]\n",
    "        dW = np.outer(A_prev, dZ)\n",
    "        db = dZ\n",
    "        \n",
    "        grads_W.insert(0, dW)\n",
    "        grads_b.insert(0, db)\n",
    "        \n",
    "\n",
    "    # Update weights and biases\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] -= learning_rate * grads_W[i]\n",
    "        biases[i] -= learning_rate * grads_b[i]\n",
    "\n",
    "    return grads_W, grads_b, weights, biases\n",
    "\n",
    "# Inputs\n",
    "X = np.array([0.6, 0.8, 0.3])\n",
    "\n",
    "Y = np.array([1, 0])\n",
    "learning_rate = 0.1 # Given\n",
    "\n",
    "weights = [\n",
    "    np.array([[0.2, 0.5, 0.3, 0.7],\n",
    "              [0.4, 0.1, 0.8, 0.2],\n",
    "              [0.6, 0.9, 0.5, 0.3]]),\n",
    "    np.array([[0.1, 0.3, 0.6, 0.8],\n",
    "              [0.5, 0.2, 0.7, 0.4],\n",
    "              [0.9, 0.8, 0.4, 0.5],\n",
    "              [0.3, 0.7, 0.2, 0.1]]),\n",
    "    np.array([[0.7, 0.5, 0.4, 0.2],\n",
    "              [0.3, 0.8, 0.6, 0.1],\n",
    "              [0.9, 0.2, 0.7, 0.3],\n",
    "              [0.5, 0.4, 0.1, 0.8]]),\n",
    "    np.array([[0.6, 0.3],\n",
    "              [0.2, 0.7],\n",
    "              [0.7, 0.5],\n",
    "              [0.4, 0.6]])\n",
    "]\n",
    "\n",
    "biases = [\n",
    "    np.array([0.2, 0.2, 0.2, 0.2]),\n",
    "    np.array([0.3, 0.3, 0.3, 0.3]),\n",
    "    np.array([0.1, 0.1, 0.1, 0.1]),\n",
    "    np.array([0.2, 0.2])\n",
    "]\n",
    "\n",
    "\n",
    "output, cache = feed_forward(X, weights, biases)\n",
    "print(\"Final Output:\", output)\n",
    "\n",
    "# Compute MSE Loss\n",
    "loss = mean_square_error(output, Y)\n",
    "print(\"\\nMean Squared Error Loss:\", loss)\n",
    "\n",
    "# Backpropagation and Update\n",
    "grads_W, grads_b, updated_weights, updated_biases = back_propagation(X, Y, weights, biases, cache, learning_rate)\n",
    "\n",
    "# Print Gradients\n",
    "print(\"\\nGradients for Weights:\")\n",
    "for i, g in enumerate(grads_W):\n",
    "    print(f\"dW{i+1}:\\n\", g)\n",
    "\n",
    "print(\"\\nGradients for Biases:\")\n",
    "for i, g in enumerate(grads_b):\n",
    "    print(f\"db{i+1}:\\n\", g)\n",
    "\n",
    "# Print updated weights\n",
    "print(\"\\nUpdated Weights:\")\n",
    "for i, w in enumerate(updated_weights):\n",
    "    print(f\"W{i+1}:\\n\", w)\n",
    "\n",
    "print(\"\\nUpdated Biases:\")\n",
    "for i, b in enumerate(updated_biases):\n",
    "    print(f\"b{i+1}:\\n\", b)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
