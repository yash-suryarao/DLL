{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c48ff86-920f-4dcb-a83a-46080e58cbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8266afb-a5da-4822-b7d4-b4693611cd2a",
   "metadata": {},
   "source": [
    "#### Q. WAP using Pre-trained Word2Vec embedding to solve the analogy ['Man':'Women'::'King':'Queen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18d536e-f1fc-4032-bebf-243c13b299ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. WAP using Pre-trained Word2Vec embedding to solve the analogy ['Man':'Women'::'King':'_____']\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load the pre-trained Word2Vec model (Google News vectors)\n",
    "print(\"Loading Word2Vec model...\")\n",
    "model = api.load(\"word2vec-google-news-300\")  # ~1.6GB download on first run\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Analogy: Man : Woman :: King : ?\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "\n",
    "print(\"Analogy Result:\")\n",
    "print(\"'Man' : 'Woman' :: 'King' :\", result[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1c703b-52f8-4ef3-af5e-0e98bdd2d6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bf7d927-ca06-4583-a7a1-7329c6fc638b",
   "metadata": {},
   "source": [
    "#### Word2Vec Without using pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e25b8a3-6b32-4c54-a72a-019775de56da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'man' : 'woman' :: 'king' : princess\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec Without using pre-trained model\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 1. Sample training corpus (you can expand this)\n",
    "corpus = [\n",
    "    ['man', 'woman', 'king', 'queen', 'boy', 'girl'],\n",
    "    ['man', 'is', 'to', 'woman'],\n",
    "    ['king', 'is', 'to', 'queen'],\n",
    "    ['prince', 'and', 'princess', 'are', 'royal'],\n",
    "    ['uncle', 'is', 'to', 'aunt'],\n",
    "    ['father', 'and', 'mother', 'are', 'parents'],\n",
    "    ['brother', 'sister', 'siblings'],\n",
    "    ['husband', 'wife', 'married']\n",
    "]\n",
    "\n",
    "# 2. Train a Word2Vec model on the above corpus\n",
    "model = Word2Vec(sentences=corpus, vector_size=100, window=2, min_count=1, sg=1)\n",
    "\n",
    "# 3. Perform the analogy: man : woman :: king : ?\n",
    "result = model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "\n",
    "# 4. Display the result\n",
    "print(\"'man' : 'woman' :: 'king' :\", result[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cae9bcd-fc8e-4a8c-973a-4654a9382954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bd6a1f7-3088-4086-8a4c-0e44b8ff7fdb",
   "metadata": {},
   "source": [
    "#### Train a Word2Vec model on a given dataset and display the most similar words to a given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abb900d6-0b52-4b15-baf7-d8ef5ef2e0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Yash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'heart':\n",
      "lifestyle: 0.1889\n",
      "smoking: 0.1886\n",
      "causes: 0.1611\n",
      "risk: 0.1599\n",
      "help: 0.1374\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download tokenizer resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Example dataset (you can replace this with your own text data)\n",
    "corpus = [\n",
    "    \"Heart disease is one of the leading causes of death.\",\n",
    "    \"Exercise and a healthy diet can reduce the risk of heart problems.\",\n",
    "    \"Blood pressure and cholesterol are important health indicators.\",\n",
    "    \"Early detection of heart disease can save lives.\",\n",
    "    \"Lifestyle changes such as quitting smoking can help prevent heart disease.\"\n",
    "]\n",
    "\n",
    "# Tokenize the sentences into words\n",
    "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save model (optional)\n",
    "model.save(\"word2vec_heart.model\")\n",
    "\n",
    "# Load model (optional)\n",
    "# model = Word2Vec.load(\"word2vec_heart.model\")\n",
    "\n",
    "# Find most similar words to a given word\n",
    "word = \"heart\"\n",
    "if word in model.wv:\n",
    "    print(f\"Most similar words to '{word}':\")\n",
    "    similar_words = model.wv.most_similar(word, topn=5)\n",
    "    for w, score in similar_words:\n",
    "        print(f\"{w}: {score:.4f}\")\n",
    "else:\n",
    "    print(f\"'{word}' not found in vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4221e1ec-50de-4597-b615-4353f9d48395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5226df8f-26dd-44d9-80a8-048ac1f80265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bac055c-ce66-450e-b3ae-2c551deafa22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8883243b-e009-4a60-9601-3f74b3a10629",
   "metadata": {},
   "source": [
    "#### Q. Write a function that encodes an input using an autoencoder and then reconstructs it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc64dc60-163f-4ad7-88a2-ffb22a33c193",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Encodes the input using the encoder of the autoencoder, then reconstructs it using the decoder.\n",
    "\n",
    "    Parameters:\n",
    "    - autoencoder: a trained autoencoder model with encoder and decoder attributes.\n",
    "    - input_tensor: a torch tensor representing the input (e.g., image or feature vector).\n",
    "\n",
    "    Returns:\n",
    "    - encoded: the latent representation.\n",
    "    - reconstructed: the reconstructed input.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88eb9bb6-d09d-41f3-8a22-13a3d25ad577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\n",
      "New Sentence:\n",
      "Machine learning and text analysis are related.\n",
      "\n",
      "Encoded Representation:\n",
      "[0.26930112 0.         0.02905112 0.12192447 0.         0.31787345\n",
      " 0.19716169 0.         0.         0.14121023 0.         0.\n",
      " 0.02682747 0.         0.38624427 0.         0.01065023 0.2929125\n",
      " 0.         0.         0.26583052 0.         0.15532437 0.\n",
      " 0.4641149  0.33174947 0.1414612  0.1612137  0.2896174  0.21501599\n",
      " 0.19792387 0.        ]\n",
      "\n",
      "Reconstructed TF-IDF Vector:\n",
      "[0.32993898 0.3596124  0.4444794  0.47059548 0.46390933 0.41327438\n",
      " 0.34619245 0.51106167 0.4292871  0.3561594  0.4550676  0.42527857\n",
      " 0.42811215 0.43288884 0.43990406]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "# Step 1: Input training sentences\n",
    "sentences = [\n",
    "    \"Machine learning is amazing.\",\n",
    "    \"Natural language processing is a part of machine learning.\",\n",
    "    \"TF-IDF is used in text analysis.\"\n",
    "]\n",
    "\n",
    "# Step 2: Create and fit TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "input_data = tfidf_matrix.toarray()\n",
    "input_dim = input_data.shape[1]\n",
    "\n",
    "# Step 3: Build and train the autoencoder\n",
    "def build_autoencoder(input_dim):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoded = Dense(32, activation='relu')(input_layer)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "    return autoencoder, encoder\n",
    "\n",
    "def encode_and_reconstruct(autoencoder, encoder, data):\n",
    "    encoded_data = encoder.predict(data)\n",
    "    reconstructed_data = autoencoder.predict(data)\n",
    "    return encoded_data, reconstructed_data\n",
    "\n",
    "autoencoder, encoder = build_autoencoder(input_dim)\n",
    "autoencoder.fit(input_data, input_data, epochs=50, verbose=0)\n",
    "\n",
    "# Step 4: Encode and reconstruct original data\n",
    "encoded_data, reconstructed_data = encode_and_reconstruct(autoencoder, encoder, input_data)\n",
    "\n",
    "# Step 5: Take a new input sentence\n",
    "new_sentence = [\"Machine learning and text analysis are related.\"]\n",
    "new_tfidf = vectorizer.transform(new_sentence).toarray()  # Use same vectorizer\n",
    "\n",
    "# Step 6: Encode and reconstruct the new sentence\n",
    "new_encoded, new_reconstructed = encode_and_reconstruct(autoencoder, encoder, new_tfidf)\n",
    "\n",
    "print(\"\\nNew Sentence:\")\n",
    "print(new_sentence[0])\n",
    "print(\"\\nEncoded Representation:\")\n",
    "print(new_encoded[0])\n",
    "print(\"\\nReconstructed TF-IDF Vector:\")\n",
    "print(new_reconstructed[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686c75f9-dc68-47ea-888b-b16252b76de5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3791807-6a16-45e6-aff4-af6834718e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "    amazing  analysis       idf        in        is  language  learning  \\\n",
      "0  0.631745  0.000000  0.000000  0.000000  0.373119  0.000000  0.480458   \n",
      "1  0.000000  0.000000  0.000000  0.000000  0.231559  0.392063  0.298174   \n",
      "2  0.000000  0.396875  0.396875  0.396875  0.234400  0.000000  0.000000   \n",
      "\n",
      "    machine   natural        of      part  processing      text        tf  \\\n",
      "0  0.480458  0.000000  0.000000  0.000000    0.000000  0.000000  0.000000   \n",
      "1  0.298174  0.392063  0.392063  0.392063    0.392063  0.000000  0.000000   \n",
      "2  0.000000  0.000000  0.000000  0.000000    0.000000  0.396875  0.396875   \n",
      "\n",
      "       used  \n",
      "0  0.000000  \n",
      "1  0.000000  \n",
      "2  0.396875  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Input sentences\n",
    "sentences = [\n",
    "    \"Machine learning is amazing.\",\n",
    "    \"Natural language processing is a part of machine learning.\",\n",
    "    \"TF-IDF is used in text analysis.\"\n",
    "]\n",
    "\n",
    "# Step 2: Create a TfidfVectorizer instance\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Step 3: Fit and transform the sentences into TF-IDF vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Step 4: Convert to a DataFrame for readability\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Step 5: Display the TF-IDF DataFrame\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72a6af6-ae72-4a90-9195-03a452c80ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
